{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4f0df21-6bb8-4f3a-84a0-5275a18b0659",
   "metadata": {},
   "source": [
    "<h1>Trabalho 2 - Regressão Linear com Múltiplas Features</h1>\n",
    "\n",
    "Nesta tarefa, implementaremos um algoritmo de regressão linear capaz de lidar com datasets de múltiplas features. Utilizaremos o método do gradiente descendente para encontrar os pesos w e b do nosso modelo.\n",
    "\n",
    "Para avaliar nosso modelo, utilizaremos curvas de aprendizado e métricas de avaliações tradicionais para regressão.\n",
    "\n",
    "Para completar esta tarefa, você deve implementar o seguinte:\n",
    "- Uma função de gradiente, para calcular o valor da derivada da função de custo para os parâmetros w e b.\n",
    "- Uma função para o método de gradiente descendente de múltiplas features.\n",
    "- Uma função de custo <b>J</b>, que será utilizada para calcular a diferença do erro quadrado.\n",
    "- Uma função que separa nosso dataset em dois conjuntos, um de treinamento e outro para validação.\n",
    "- Gerar visualizações de curvas de aprendizado.\n",
    "\n",
    "O dataset que utilizaremos está disponível no moodle. É novamente um dataset de casas, porém agora considerando outras características de uma casa para considerar seu valor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dd4d13-7c97-4800-a260-6cc0a028726e",
   "metadata": {},
   "source": [
    "Trabalharemos basicamente com arrays numpy, porém utilizaremos a biblioteca pandas pela conveniência de informações e facilidade de carregar o dataset disponibilizado em um arquivo CSV.\n",
    "Também utilizaremos o módulo pyplot da biblioteca matplotlib, para plotar os gráficos que utilizaremos para avaliar o modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e147faa5-4a54-4805-a7aa-5297c0446bfe",
   "metadata": {},
   "source": [
    "<h2>Importando biliotecas, carregando e preparando dataset</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25157cf1-64fa-4fdb-97b2-bf4134de82d8",
   "metadata": {},
   "source": [
    "Importando as bibliotecas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "80734f5b-7f14-4511-926c-ceb995016ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01badec5-9d12-45d3-81e6-f10c8ab6dd86",
   "metadata": {},
   "source": [
    "O primeiro passo é importar nosso dataset em um dataframe pandas. \n",
    "\n",
    "Note que se o dataset não estivar na mesma pasta em que este Notebook você precisará alterar o caminho para o arquivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9cb3bce2-5815-4bd6-b659-b2906fb8e45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"casas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8eb781-e69e-4a39-adcf-d1778dfc35f2",
   "metadata": {},
   "source": [
    "O dataset que vamos utilizar contém exemplos de casas e para casa temos sua idade (em anos), a distância para escola mais próxima (em kilômetros), o número de lojas de conveniência em um raio de 1 km e o valor da casa (em milhares de reais).\n",
    "\n",
    "Neste trabalho, o objetivo é treinar um modelo capaz de predizer o <b>preço</b> da casa com base em sua idade, distância para escola e número de lojas de conveniência próximas a ela.\n",
    "\n",
    "Inicialmente, vamos analisar os 10 primeiros exemplos do nosso dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "54c91c86-f97e-4453-b1b3-1c4eccbe4ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IdadeCasa-Anos</th>\n",
       "      <th>DistanciaEscola-Kilometros</th>\n",
       "      <th>Num_Lojas_Conveniencia</th>\n",
       "      <th>Preco-MilReais</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.0</td>\n",
       "      <td>0.084</td>\n",
       "      <td>10</td>\n",
       "      <td>37.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.5</td>\n",
       "      <td>0.306</td>\n",
       "      <td>9</td>\n",
       "      <td>42.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.3</td>\n",
       "      <td>0.561</td>\n",
       "      <td>5</td>\n",
       "      <td>47.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.3</td>\n",
       "      <td>0.561</td>\n",
       "      <td>5</td>\n",
       "      <td>54.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.390</td>\n",
       "      <td>5</td>\n",
       "      <td>43.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.1</td>\n",
       "      <td>2.175</td>\n",
       "      <td>3</td>\n",
       "      <td>32.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>34.5</td>\n",
       "      <td>0.623</td>\n",
       "      <td>7</td>\n",
       "      <td>40.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20.3</td>\n",
       "      <td>0.287</td>\n",
       "      <td>6</td>\n",
       "      <td>46.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31.7</td>\n",
       "      <td>5.512</td>\n",
       "      <td>1</td>\n",
       "      <td>18.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17.9</td>\n",
       "      <td>1.783</td>\n",
       "      <td>3</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IdadeCasa-Anos  DistanciaEscola-Kilometros  Num_Lojas_Conveniencia  \\\n",
       "0            32.0                       0.084                      10   \n",
       "1            19.5                       0.306                       9   \n",
       "2            13.3                       0.561                       5   \n",
       "3            13.3                       0.561                       5   \n",
       "4             5.0                       0.390                       5   \n",
       "5             7.1                       2.175                       3   \n",
       "6            34.5                       0.623                       7   \n",
       "7            20.3                       0.287                       6   \n",
       "8            31.7                       5.512                       1   \n",
       "9            17.9                       1.783                       3   \n",
       "\n",
       "   Preco-MilReais  \n",
       "0            37.9  \n",
       "1            42.2  \n",
       "2            47.3  \n",
       "3            54.8  \n",
       "4            43.1  \n",
       "5            32.1  \n",
       "6            40.3  \n",
       "7            46.7  \n",
       "8            18.8  \n",
       "9            22.1  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e4017b-4fed-4800-acc0-f768b9d140c1",
   "metadata": {},
   "source": [
    "A biblioteca pandas possui a função <b>describe()</b>, que compila diversas medidas estatísticas de cada feature do nosso dataset de forma automática.\n",
    "- count: quantidade de exemplos no dataset\n",
    "- mean: média aritmética\n",
    "- std: desvio padrão\n",
    "- min: menor valor da feature no dataset\n",
    "- 25% / 50% / 75%: percentis para cada feature (valor que divide o dataset nessa proporção, para 50% é o mesmo que a mediana)\n",
    "- max: maior valor da feature no dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "40d96503-2d4f-4960-ac91-d3d3f2d95f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IdadeCasa-Anos</th>\n",
       "      <th>DistanciaEscola-Kilometros</th>\n",
       "      <th>Num_Lojas_Conveniencia</th>\n",
       "      <th>Preco-MilReais</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>414.000000</td>\n",
       "      <td>414.000000</td>\n",
       "      <td>414.000000</td>\n",
       "      <td>414.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>17.712560</td>\n",
       "      <td>1.083377</td>\n",
       "      <td>4.094203</td>\n",
       "      <td>37.980193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.392485</td>\n",
       "      <td>1.262159</td>\n",
       "      <td>2.945562</td>\n",
       "      <td>13.606488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.025000</td>\n",
       "      <td>0.289000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>27.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>16.100000</td>\n",
       "      <td>0.492000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>38.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>28.150000</td>\n",
       "      <td>1.453500</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>46.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>43.800000</td>\n",
       "      <td>6.488000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>117.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       IdadeCasa-Anos  DistanciaEscola-Kilometros  Num_Lojas_Conveniencia  \\\n",
       "count      414.000000                  414.000000              414.000000   \n",
       "mean        17.712560                    1.083377                4.094203   \n",
       "std         11.392485                    1.262159                2.945562   \n",
       "min          0.000000                    0.023000                0.000000   \n",
       "25%          9.025000                    0.289000                1.000000   \n",
       "50%         16.100000                    0.492000                4.000000   \n",
       "75%         28.150000                    1.453500                6.000000   \n",
       "max         43.800000                    6.488000               10.000000   \n",
       "\n",
       "       Preco-MilReais  \n",
       "count      414.000000  \n",
       "mean        37.980193  \n",
       "std         13.606488  \n",
       "min          7.600000  \n",
       "25%         27.700000  \n",
       "50%         38.450000  \n",
       "75%         46.600000  \n",
       "max        117.500000  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7543a4e8-9359-416b-8573-bcf49478888e",
   "metadata": {},
   "source": [
    "Para implementação, iremos trabalhar apenas com arrays da bilioteca numpy, por isso converteremos o dataframe pandas em dois arrays numpy. O primeiro contendo nossos exemplos e o segundo contendo os nossos valores objetivos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1a65f3bd-848f-486a-a549-4586daaf857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nparray = dataset.to_numpy()\n",
    "X = nparray[:,:3]\n",
    "Y = nparray[:,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fcd837-f689-4dd3-8291-8f93d80bed98",
   "metadata": {},
   "source": [
    "Abaixo podemos verificar as dimensões e tipo dos nossos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8a6ca0a6-3827-4560-b8af-7d32a0fb1b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(414, 3)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4b22465e-dd61-45a5-97fb-f1f95a729d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32.   ,  0.084, 10.   ],\n",
       "       [19.5  ,  0.306,  9.   ],\n",
       "       [13.3  ,  0.561,  5.   ],\n",
       "       ...,\n",
       "       [18.8  ,  0.39 ,  7.   ],\n",
       "       [ 8.1  ,  0.104,  5.   ],\n",
       "       [ 6.5  ,  0.09 ,  9.   ]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "eb776194-bdc0-42df-8aac-751bb422bd49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(414,)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e0306956-8e94-4c06-a088-5c7c721ae0b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 37.9,  42.2,  47.3,  54.8,  43.1,  32.1,  40.3,  46.7,  18.8,\n",
       "        22.1,  41.4,  58.1,  39.3,  23.8,  34.3,  50.5,  70.1,  37.4,\n",
       "        42.3,  47.7,  29.3,  51.6,  24.6,  47.9,  38.8,  27. ,  56.2,\n",
       "        33.6,  47. ,  57.1,  22.1,  25. ,  34.2,  49.3,  55.1,  27.3,\n",
       "        22.9,  25.3,  47.7,  46.2,  15.9,  18.2,  34.7,  34.1,  53.9,\n",
       "        38.3,  42. ,  61.5,  13.4,  13.2,  44.2,  20.7,  27. ,  38.9,\n",
       "        51.7,  13.7,  41.9,  53.5,  22.6,  42.4,  21.3,  63.2,  27.7,\n",
       "        55. ,  25.3,  44.3,  50.7,  56.8,  36.2,  42. ,  59. ,  40.8,\n",
       "        36.3,  20. ,  54.4,  29.5,  36.8,  25.6,  29.8,  26.5,  40.3,\n",
       "        36.8,  48.1,  17.7,  43.7,  50.8,  27. ,  18.3,  48. ,  25.3,\n",
       "        45.4,  43.2,  21.8,  16.1,  41. ,  51.8,  59.5,  34.6,  51. ,\n",
       "        62.2,  38.2,  32.9,  54.4,  45.7,  30.5,  71. ,  47.1,  26.6,\n",
       "        34.1,  28.4,  51.6,  39.4,  23.1,   7.6,  53.3,  46.4,  12.2,\n",
       "        13. ,  30.6,  59.6,  31.3,  48. ,  32.5,  45.5,  57.4,  48.6,\n",
       "        62.9,  55. ,  60.7,  41. ,  37.5,  30.7,  37.5,  39.5,  42.2,\n",
       "        20.8,  46.8,  47.4,  43.5,  42.5,  51.4,  28.9,  37.5,  40.1,\n",
       "        28.4,  45.5,  52.2,  43.2,  45.1,  39.7,  48.5,  44.7,  28.9,\n",
       "        40.9,  20.7,  15.6,  18.3,  35.6,  39.4,  37.4,  57.8,  39.6,\n",
       "        11.6,  55.5,  55.2,  30.6,  73.6,  43.4,  37.4,  23.5,  14.4,\n",
       "        58.8,  58.1,  35.1,  45.2,  36.5,  19.2,  42. ,  36.7,  42.6,\n",
       "        15.5,  55.9,  23.6,  18.8,  21.8,  21.5,  25.7,  22. ,  44.3,\n",
       "        20.5,  42.3,  37.8,  42.7,  49.3,  29.3,  34.6,  36.6,  48.2,\n",
       "        39.1,  31.6,  25.5,  45.9,  31.5,  46.1,  26.6,  21.4,  44. ,\n",
       "        34.2,  26.2,  40.9,  52.2,  43.5,  31.1,  58. ,  20.9,  48.1,\n",
       "        39.7,  40.8,  43.8,  40.2,  78.3,  38.5,  48.5,  42.3,  46. ,\n",
       "        49. ,  12.8,  40.2,  46.6,  19. ,  33.4,  14.7,  17.4,  32.4,\n",
       "        23.9,  39.3,  61.9,  39. ,  40.6,  29.7,  28.8,  41.4,  33.4,\n",
       "        48.2,  21.7,  40.8,  40.6,  23.1,  22.3,  15. ,  30. ,  13.8,\n",
       "        52.7,  25.9,  51.8,  17.4,  26.5,  43.9,  63.3,  28.8,  30.7,\n",
       "        24.4,  53. ,  31.7,  40.6,  38.1,  23.7,  41.1,  40.1,  23. ,\n",
       "       117.5,  26.5,  40.5,  29.3,  41. ,  49.7,  34. ,  27.7,  44. ,\n",
       "        31.1,  45.4,  44.8,  25.6,  23.5,  34.4,  55.3,  56.3,  32.9,\n",
       "        51. ,  44.5,  37. ,  54.4,  24.5,  42.5,  38.1,  21.8,  34.1,\n",
       "        28.5,  16.7,  46.1,  36.9,  35.7,  23.2,  38.4,  29.4,  55. ,\n",
       "        50.2,  24.7,  53. ,  19.1,  24.7,  42.2,  78. ,  42.8,  41.6,\n",
       "        27.3,  42. ,  37.5,  49.8,  26.9,  18.6,  37.7,  33.1,  42.5,\n",
       "        31.3,  38.1,  62.1,  36.7,  23.6,  19.2,  12.8,  15.6,  39.6,\n",
       "        38.4,  22.8,  36.5,  35.6,  30.9,  36.3,  50.4,  42.9,  37. ,\n",
       "        53.5,  46.6,  41.2,  37.9,  30.8,  11.2,  53.7,  47. ,  42.3,\n",
       "        28.6,  25.7,  31.3,  30.1,  60.7,  45.3,  44.9,  45.1,  24.7,\n",
       "        47.1,  63.3,  40. ,  48. ,  33.1,  29.5,  24.8,  20.9,  43.1,\n",
       "        22.8,  42.1,  51.7,  41.5,  52.2,  49.5,  23.8,  30.5,  56.8,\n",
       "        37.4,  69.7,  53.3,  47.3,  29.3,  40.3,  12.9,  46.6,  55.3,\n",
       "        25.6,  27.3,  67.7,  38.6,  31.3,  35.3,  40.3,  24.7,  42.5,\n",
       "        31.9,  32.2,  23. ,  37.3,  35.5,  27.7,  28.5,  39.7,  41.2,\n",
       "        37.2,  40.5,  22.3,  28.1,  15.4,  50. ,  40.6,  52.5,  63.9])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f65bca9-b14c-4648-8217-d55a3d34af9f",
   "metadata": {},
   "source": [
    "<h2>Desenvolvendo gradiente descendente</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b2e908-384a-4f80-b401-66aa8c5fb5bf",
   "metadata": {},
   "source": [
    "Antes de poder treinar um modelo com o nosso dataset, precisamos desenvolver a função de custo, função de cálculo de gradiente e a função para executar o método do gradiente descendente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c854168-1d07-4f76-b02b-892438a08061",
   "metadata": {},
   "source": [
    "<h3>Função de custo</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14992f5e-32b2-4399-81f1-02c280775897",
   "metadata": {},
   "source": [
    "A primeira função a ser desenvolvida é a função $J(\\vec{w},b)$ para computar o custo do erro do modelo no dataset.\n",
    "\n",
    "Para calcular o erro utilizaremos a função do erro quadrático médio (Mean Squared Error - MSE).\n",
    "\n",
    "A MSE é dada pela seguinte equação:\n",
    "\n",
    "$$\n",
    "    (\\hat{y} - y)^2\n",
    "$$\n",
    "\n",
    "Onde $\\hat{y}$ é o valor predito e $y$ é o valor alvo.\n",
    "\n",
    "O custo de erro total (custo sobre o dataset inteiro) é dado pela seguinte equação:\n",
    "\n",
    "$$\n",
    "1/2m\\sum_{i=0}^{m-1} ((\\hat{y}^{(i)} - y^{(i)})^2)\n",
    "$$\n",
    "\n",
    "Como estamos agora trabalhando com múltiplas features, nossa função agora é a seguinte:\n",
    "$$\n",
    "\\hat{y}^{(i)} = f_{\\vec{w},b}(\\vec{x}^{(i)}) = \\vec{w}\\cdot \\vec{x}^{(i)}+b\n",
    "$$\n",
    "\n",
    "Onde $\\vec{w}$ é um vetor com parâmetros $w_0$...$w_n$ e $\\vec{x}^{(i)}$ são os valores das features ${x_0}^{(i)}...{x_n}^{(i)}$ para o i-ésimo exemplo do dataset.\n",
    "\n",
    "<b><i>ATENÇÃO!!!!!!!</i></b> $x^{(i)}$ não significa $x$ <b>elevado</b> ao número $i$, mas sim o i-ésimo exemplo do nosso dataset!\n",
    "\n",
    "Lembrando que o que estamos calculando entre $\\vec{w}$ e $\\vec{x_{i}}$ é o produto escalar (dot product) entre os dois vetores, representado por $\\vec{w}\\cdot \\vec{x_{i}}$\n",
    "\n",
    "O produto escalar entre um vetor $w$ e um vetor $x$ é calculado com a seguinte fórmula:\n",
    "$$\n",
    "\\sum_{j=0}^{n-1}w_j \\times x_j\n",
    "$$\n",
    "Que é equivalente a:\n",
    "$$\n",
    "w_0 \\times x_0 + w_1 \\times x_1 + w_2 \\times x_2 + ... + w_n \\times x_n\n",
    "$$\n",
    "\n",
    "<b>Dica:</b> Podemos utilizar a função <b>numpy.dot(w,x)</b> para calcular isto de forma vetorizada (em paralelo) e acelerar - e muito - nossos cálculos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5a96689c-5931-428f-95d2-c3209b5d5de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcao_custo(X,Y,W,b):\n",
    "    \"\"\"\n",
    "    Parâmetros:\n",
    "    X (ndarray (m,n)) : Dataset, m exemplos com n features\n",
    "    Y (ndarray (m,))  : valores objetivo para cada exemplo do dataset\n",
    "    W (ndarray (n,))  : parâmetros w do modelo\n",
    "    b (escalar)       : parâmetro b do modelo\n",
    "    \n",
    "    Retorno:\n",
    "    custo_total (escalar): custo total dos parâmetros w e b no dataset X\n",
    "    \"\"\"\n",
    "    custo_total = 0\n",
    "    m,n = X.shape #m = total de exemplos, n = total de features\n",
    "\n",
    "    #código inicia aqui\n",
    "\n",
    "    # X(i) =conjunto de dados na posicao i do dataset x\n",
    "\n",
    "    # esc = np.dot(X, W) + b   \n",
    "        \n",
    "    for i in range(m):\n",
    "        esc = np.dot(X[i], W)\n",
    "        yPred = esc + b\n",
    "        custo_total += (yPred - Y[i])**2\n",
    "\n",
    "    custo_total = (1/(2*m))*custo_total\n",
    "\n",
    "    #código termina aqui\n",
    "    return custo_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947cf107-cb60-44f0-8acd-e0fb9bef9206",
   "metadata": {},
   "source": [
    "<h3>Calculando gradiente</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b54635-5633-4976-ab09-4436fb2d2a67",
   "metadata": {},
   "source": [
    "A função gradiente calcula o valor das derivadas da função de custo $J(\\vec{w},b)$ em relação aos parâmetros $\\vec{w}$ e $b$.\n",
    "\n",
    "Quando temos múltiplos parâmetros precisamos calcular individualmente a derivada de cada parâmetro e armazenar todos esses valores em um vetor (o vetor gradiente!)\n",
    "\n",
    "A derivada da função de custo em relação a um parâmetro $w_j$ é: \n",
    "\n",
    "$$\n",
    "1/m\\sum_{i=0}^{m-1} (\\hat{y_{i}} - y_{i})\\times {x_j}^{(i)}\n",
    "$$\n",
    "\n",
    "A derivada da função de custo em relação a **b** é:\n",
    "\n",
    "$$\n",
    "1/m\\sum_{i=1}^m (\\hat{y_{i}} - y_{i})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4c56eca4-d4d9-40a7-b861-165cdb2e5b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiente(X,Y,W,b):\n",
    "    \"\"\"\n",
    "    Função para computar o gradiente com parâmetros específicos w e b\n",
    "    Parâmetros:\n",
    "    X (ndarray (m,n)): Dataset, m exemplos com n features\n",
    "    Y (ndarray (m,)) : valores objetivo\n",
    "    W (ndarray (n,)) : vetor de parâmetros w do modelo\n",
    "    b (escalar)      : parâmetro b do modelo\n",
    "\n",
    "    Retorno:\n",
    "    derivada_w (ndarray (n,)): valor da derivada da função de custo em relação a cada parâmetro wj\n",
    "    derivada_b (escalar):      valor da derivada da função de custo em relação a b\n",
    "    \"\"\"\n",
    "\n",
    "    m,n = X.shape #m = total de exemplos, n = total de features\n",
    "    derivadas_w = np.zeros(n) #array de tamanho n inicializado com zeros\n",
    "    derivada_b = 0\n",
    "\n",
    "    #código inicia aqui\n",
    "        \n",
    "    for i in range(m):\n",
    "        esc = np.dot(X[i], W)\n",
    "        yPred = esc + b\n",
    "        derivadas_w += (yPred - Y[i]) * X[i]\n",
    "        derivada_b += (yPred - Y[i])\n",
    "            #não esqueça de utilizar a função np.dot() para cálculo do produto escalar!\n",
    "    \n",
    "    #código termina aqui\n",
    "\n",
    "    return derivadas_w, derivada_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703e7c49-f068-4f40-ac38-79467c7aa567",
   "metadata": {},
   "source": [
    "<h3>Método do gradiente descendente</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465bd0f5-71d0-4b66-bec1-ac47126b345d",
   "metadata": {},
   "source": [
    "O método **gradiente_descendente** minimiza o custo para os parâmetros $\\vec{w}$ e $b$.\n",
    "\n",
    "O gradiente descendente funciona como abaixo.\n",
    "\n",
    "Repetir até convergir:\n",
    "\n",
    "{\n",
    "$$w_0 = w_0 - \\alpha * \\frac{\\partial J}{\\partial w_0}$$\n",
    "$$w_0 = w_1 - \\alpha * \\frac{\\partial J}{\\partial w_1}$$\n",
    "$$.$$\n",
    "$$.$$\n",
    "$$.$$\n",
    "$$w_0 = w_j - \\alpha * \\frac{\\partial J}{\\partial w_j}$$\n",
    "$$b = b - \\alpha * derivada_b$$\n",
    "}\n",
    "\n",
    "Lembrando que $\\vec{w}$ e $b$ precisam ser atualizados simultaneamente. Ou seja, suas derivadas precisam ser **todas** calculadas antes de atuliazar qualquer um deles.\n",
    "\n",
    "**Convergir** significa que nenhum dos parâmetro é atualizado após uma iteração do método, o que significa que já encontramos um mínimo global. Na prática, isto pode demorar muito, por isso temos um novo parâmetro, chamado de ***épocas***, que define um número máximo de iterações para o método, para evitar que fique tentando convergir infinitamente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f0abecd9-fa76-410f-ad29-3671ee5b3e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiente_descendente(X,Y,W_inicial,b_inicial,learning_rate,epocas):\n",
    "    \"\"\"\n",
    "    Função para encontrar os parâmetros w e b que minimizam o custo de erro do nosso modelo\n",
    "    \n",
    "    Parâmetros:\n",
    "    X (ndarray (m,n))        : Dataset, m exemplos com n features\n",
    "    Y (ndarray (m,))         : valores objetivo\n",
    "    W_inicial (ndarray (n,))      : vetor de parâmetros w para começar o método\n",
    "    b_inicial (escalar)      : parâmetro b inicial para começar o método\n",
    "    learning_rate (escalar)  : taxa de aprendizado do nosso algoritmo (geralmente menor que 0.01)\n",
    "    epocas(escalar)          : numero máximo de iterações para o método do gradiente descente\n",
    "    \n",
    "    Retorno:\n",
    "    W (ndarray (n,))         : vetor com melhores valores encontrados para os parâmetros w\n",
    "    b (escalar)              : melhor valor encontrado para b\n",
    "    histórico_J (list)       : uma lista contendo os valores da função de custo ao longo das épocas\n",
    "    \"\"\"\n",
    "    \n",
    "    m,n = X.shape #m = total de exemplos, n total de features\n",
    "\n",
    "    W = W_inicial\n",
    "    b = b_inicial\n",
    "\n",
    "    historico_J = []    #pode ser interessante calcular o custo a cada X épocas, ao invés de calcular em toda época, por questões de performance\n",
    "    \n",
    "    #código inicia aqui\n",
    "\n",
    "    for i in range(epocas):\n",
    "        auxW = W.copy()\n",
    "        auxB = b\n",
    "\n",
    "        derivadas_w, derivada_b = gradiente(X,Y,W,b)\n",
    "        \n",
    "        W -= learning_rate * derivadas_w\n",
    "        b -= learning_rate * derivada_b\n",
    "        historico_J.append(funcao_custo(X,Y,W,b))\n",
    "\n",
    "        if np.all(W == auxW) and b == auxB:\n",
    "           break\n",
    "    \n",
    "    #código encerra aqui\n",
    "\n",
    "    return W,b,historico_J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2662c3c7-1048-4712-9667-3ac73dc1b8f7",
   "metadata": {},
   "source": [
    "<h2>Treinando e avaliando nosso modelo</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4107e303-4f43-4b7c-a7a4-04901b899595",
   "metadata": {},
   "source": [
    "Quando treinamos um modelo, idealmente desejamos que ele tenha boa performance em dados que ele não utilizou durante o treinamento.\n",
    "Como dados não caem do céu, em geral o que fazemos é dividir nosso conjunto de treinamento em duas partes, uma para ser utilizada no treinamento (o conjunto de treinamento de fato) e outra para ser utilizada para avaliar o modelo (o conjunto de testes).\n",
    "\n",
    "Durante o treinamento o algoritmo não deve utilizar **nenhum** dado de teste. Só assim poderemos ter certeza de que nosso modelo performa bem em dados que ele não conhece.\n",
    "\n",
    "Nossa próxima tarefa é desenvolver uma função que divida nosso dataset em duas partes de forma aleatória.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce97b54-a08b-45d2-b2a5-410adeacd568",
   "metadata": {},
   "source": [
    "<h3>Separando nosso dataset</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a981cc9-2f73-4a6a-bfde-19c7e8c7c40c",
   "metadata": {},
   "source": [
    "A função **divide_dataset()** deve receber um conjunto de dados anotados $X,Y$, um valor entre 0 e 1 que define qual será o tamanho do conjunto de treino (em %) e um valor para ser a seed do nosso gerador de números aleatórios.\n",
    "\n",
    "A função deve dividir o dataset recebido em dois datasets, respeitando o tamanho da divisão que foi passada por parâmetro. Cada novo dataset (treino/teste) deve ser populado com exemplos **aleatórios** retirados do dataset original. A seed aleatória passada como parâmetro é para garantir reprodutibilidade.\n",
    "\n",
    "**ATENÇÃO:** Não esqueça que os índices dos exemplos e dos valores objetivos devem ser mantidos iguais para garantir que cada exemplo de casa tenha seu preço correto!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "81aac6b4-94a5-418f-9723-ac56d1c6c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_dataset(X,Y,fator_de_divisao,semente_random):\n",
    "    \"\"\"\n",
    "    Parâmetros:\n",
    "    X (ndarray (m,n))         : Dataset, m exemplos com n features\n",
    "    Y (ndarray (m,))          : valores objetivo para cada exemplo do dataset\n",
    "    fator_de_divisao (float)  : tamanho da particição do dataset\n",
    "    b (escalar)               : parâmetro b do modelo\n",
    "    \n",
    "    Retorno:\n",
    "    X_train (ndarray (m*fator_de_divisao,n))      : Dataset com exemplos a serem utilizados em treino\n",
    "    Y_train (ndarray (m*fator_de_divisao,))       : Valores objetivo dos exemplos a serem utilizados em treino\n",
    "    X_test  (ndarray (m*(1-fator_de_divisao),n))  : Dataset com exemplos a serem utilizados em teste\n",
    "    Y_test  (ndarray (m*(1-fator_de_divisao),))   : Valores objetivo dos exemplos a serem utilizados em teste\n",
    "    \"\"\"\n",
    "\n",
    "    #código inicia aqui\n",
    "\n",
    "    np.random.seed(semente_random)\n",
    "    np.random.shuffle(X)\n",
    "\n",
    "    np.random.seed(semente_random)\n",
    "    np.random.shuffle(Y)\n",
    "\n",
    "    m,n = X.shape\n",
    "\n",
    "    # X_treino = X[:int(m*fator_de_divisao)]\n",
    "    # Y_treino = Y[:int(m*fator_de_divisao)]\n",
    "\n",
    "    # X_teste = X[int(n*fator_de_divisao):]\n",
    "    # Y_teste = Y[int(n*fator_de_divisao):]\n",
    "\n",
    "    X_treino = np.array(X[:int(m*fator_de_divisao)])\n",
    "    Y_treino = np.array(Y[:int(m*fator_de_divisao)])\n",
    "\n",
    "    X_teste = np.array(X[int(m*fator_de_divisao):])\n",
    "    Y_teste = np.array(Y[int(m*fator_de_divisao):])\n",
    "\n",
    "    # X_treino = np.ndarray((int(m*fator_de_divisao), n))\n",
    "    # Y_treino = np.ndarray((int(m*fator_de_divisao),))\n",
    "    # X_teste = np.ndarray((int(m*(1-fator_de_divisao)), n))\n",
    "    # Y_teste = np.ndarray((int(m*(1-fator_de_divisao)),))\n",
    "\n",
    "    return X_treino, Y_treino, X_teste, Y_teste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121cb037-3223-43c5-8e28-02e384c96e93",
   "metadata": {},
   "source": [
    "<h3>Treinando o modelo</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5f7723-62b4-4808-9b36-06dbe8a142f3",
   "metadata": {},
   "source": [
    "Com todas funções necessárias implementadas, podemos iniciar o nosso treinamento.\n",
    "\n",
    "Abaixo estão alguns valores para realizarmos um treinamento inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7195a485-e062-4fb7-8e95-791427e3ed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.00001\n",
    "epocas = 500\n",
    "W_inicial = np.zeros(X.shape[1])\n",
    "b = 0\n",
    "\n",
    "X_treino, Y_treino, X_teste, Y_teste = divide_dataset(X,Y,0.75,10)\n",
    "\n",
    "W, b, historico_J = gradiente_descendente(X_treino, Y_treino, W_inicial, b, learning_rate, epocas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fc8c29-eede-4724-b922-134ceface1e3",
   "metadata": {},
   "source": [
    "Vamos verificar quais valores nossa execução encontrou para os parâmetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3cc5c26b-591c-433f-b23f-1d0c8d883733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ffc00d84-4f24-4ff4-977e-e8fd84aebf3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ff41ba-421e-4aff-985b-e69dd0a4bc4d",
   "metadata": {},
   "source": [
    "<h3>Avaliando o modelo</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57960d89-966d-4b0a-8802-174234007a93",
   "metadata": {},
   "source": [
    "Existem diversas formas e métricas para se avaliar um modelo, dependendo de qual tipo de modelo, treinamento ou tarefa ele está realizando.\n",
    "\n",
    "A forma mais básica de avaliação é, depois de termos um modelo treinado, verificar a sua performance para um conjunto de dados de teste. Se o modelo possuir uma performance razoável em um conjunto em que ele nunca treinou, isto pode ser um indicativo de que temos um bom modelo em mãos.\n",
    "\n",
    "Entretanto, algumas vezes podemos nos deparar com um modelo que tenha uma performance ruim em dados de teste, e então investigações mais profundas são necessárias. Tipicamente analisamos **curvas de aprendizado** para poder compreender melhor o processo de treinamento do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4224f3-6936-4f39-ae21-68dbc1450901",
   "metadata": {},
   "source": [
    "<h5>Métricas de performance</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd788415-3efc-4157-aa7c-6f8bca367f5b",
   "metadata": {},
   "source": [
    "A métrica de performance a ser utilizada depende do tipo de tarefa realizada pelo modelo. Na tarefa de regressão, queremos predizer um valor numérico que dificilmente será exatamente igual ao valor objetivo. Portanto, medidas que considerem a diferença entre o valor predito e o valor objetivo são utilizadas para avaliar um modelo de regressão.\n",
    "\n",
    "As medidas mais comuns são:\n",
    "- Média do erro quadrado (MSE)\n",
    "- Raíz da média do erro quadrado (RMSE)\n",
    "- Média do erro absoluto (MAE)\n",
    "\n",
    "A MSE é a mesma função que utilizamos em nossa função de custo no treinamento. Ela tem como características sempre resultar um erro positivo (pois eleva o erro ao quadrado) e de \"inflar\" o valor do erro a medida que quanto maior a diferença entre o valor predito e o valor esperado, maior será o erro quadrático.\n",
    "\n",
    "A RMSE é semelhante à MSE, visto que resulta do cálculo da raíz quadrada da MSE. Ela também possui a característica de que erros serão sempre positivos (pois são elevados ao quadrado), e \"infla\" erros maiores. Porém, calculando a raíz quadrada o resultado é de que o valor da RMSE está na mesma grandeza do que o nosso valor esperado (e não ao quadrado, como na MSE). Para o dataset de casas poderíamos interpretar o valor da RMSE como o custo do erro médio em reais, por exemplo.\n",
    "\n",
    "A MAE é uma média da diferença absoluta entre o valor previsto e o valor esperado. Diferente da MSE e da RMSE, ela não eleva o erro ao quadrado e, com isso, temos uma medida linear em que o erro não é inflado a medida que fica mais distante do valor esperado. Assim como a RMSE, ela também tem como resultado um valor na mesma unidade de grandeza do nosso valor esperado.\n",
    "\n",
    "Em geral, utilizamos métricas quadradas para penalizar mais o modelo a medida que o erro vai ficando maior (predição vai se distanciando do valor esperado). Para treinamento é comum utilizarmos a MSE, enquanto para avaliar o modelo é mais comum utilizarmos a RMSE pois a interpretação do seu valor é direto (mesma grandeza)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9809620-eab8-41f1-b0ce-1cdd67fb81d9",
   "metadata": {},
   "source": [
    "Com base nos parâmetros $\\vec{w}$ e $b$ que encontramos, podemos utilizar a função de custo já desenvolvida para verificar a MSE em nosso conjunto testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e29a38a6-bde8-49f4-8833-fbb0ff0e98c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "erro_medio_teste = funcao_custo(X_teste,Y_teste,W,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de131e6e-d152-4a6c-83cf-c29afcf4f691",
   "metadata": {},
   "source": [
    "Podemos interpretar o valor acima como sendo a média do valor quadrado em reais do quão distante do valor alvo o nosso modelo previu o valor da casa.\n",
    "\n",
    "Se quisermos interpretar esse valor na mesma grandeza do que nosso atributo alvo, podemos calcular a RMSE, basta calcularmos a raíz quadrada da MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "73eb2bc3-6765-47bb-9aa2-2385aa5dab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_teste = np.sqrt(erro_medio_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bcad35-d98c-4b59-84ef-c482dde65451",
   "metadata": {},
   "source": [
    "Idealmente, se nosso modelo fosse perfeito, o valor dessas métricas deveria ser 0. No mundo real, isso dificilmente irá acontecer. Então se um valor de erro é bom ou ruim, vai depender da tarefa e do quão disposto o modelador está em aceitar erros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eed73a-38a1-48ed-9f0a-d11c8238f465",
   "metadata": {},
   "source": [
    "<h4>Curvas de aprendizado</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4398063-0234-4047-bce1-e4520d4003f4",
   "metadata": {},
   "source": [
    "Para avaliar a performance, é muito comum utilizarmos **curvas de treinamento** (learning curves). Uma curva de treinamento é um plot da performance de um modelo sobre a experiência ou sobre o tempo.\n",
    "\n",
    "Avaliar curvas de treinamento durante o treinamento nos permite identificar problemas como **underfitting** ou **overfitting**, ou mesmo se o modelo está bem ajustado.\n",
    "\n",
    "A performance é medida utilizando alguma métrica que faça sentido para o problema em questão, como erro quadrado ou entropia-cruzada binária."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d573d770-d392-40e2-848d-fac8832f1c47",
   "metadata": {},
   "source": [
    "<h5>Analisando convergência</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b6d0e5-637c-484f-96bd-5df4ee14353a",
   "metadata": {},
   "source": [
    "Uma primeira avaliação que podemos fazer em nosso modelo de regressão utilizando o gradiente descendente é analisar a convergência do método ao longo do tempo. O objetivo do gradiente descendente é minimizar a função custo para o conjunto de dados utilizado no treinamento. Sendo assim, é razoável esperar que o valor da função de custo diminua a medida que o método execute iterações.\n",
    "\n",
    "Caso o valor do custo durante o treinamento se mantenha alto ao longo do tempo, este é um bom indicativo de que nosso modelo está sofrendo de **underfitting**. Em outras palavras, nosso modelo não consegue aprender bem nossos dados.\n",
    "\n",
    "Abaixo vamos plotar um gráfico que nos mostra a evolução do custo ao longo do nosso treinamento. O esperado é que o custo comece alto e vá diminuindo ao longo do tempo. Isto significa que a medida que treinamos estamos de fato conseguindo diminuir o nosso erro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1d751853-fab0-46b2-a77f-d325505a3692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x17062ba9490>]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.ylabel(\"Custo\") #nome do eixo y\n",
    "plt.xlabel(\"Tempo de Treinamento\") #nome do deixo x\n",
    "plt.plot(historico_J)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9f083a-dbae-4b20-8862-d0867b1b768c",
   "metadata": {},
   "source": [
    "<h5>Outras curvas de aprendizado</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6a0d54-07d1-4291-9564-8ce3d27decbe",
   "metadata": {},
   "source": [
    "Analisar uma curva de convergência é útil para saber se nosso modelo ao menos está conseguindo melhorar a medida que treina, porém ainda nos diz muito pouco sobre a performance do nosso modelo no mundo real.\n",
    "\n",
    "Em geral, é útil plotarmos em conjunto curvas que mostrem o erro do modelo no conjunto de treino e no conjunto de validação ou teste. Comparando essas curvas, podemos chegar a diversas conclusões. Abaixo alguns exemplos:\n",
    "\n",
    "- Se ambas curvas diminuem ao longo do tempo e tendem a se aproximar, então temos um bom modelo.\n",
    "- Se o erro do conjunto de treinamento diminui, porém o erro do conjunto de teste não modifica, isto pode indicar que estamos tendo **overfitting**, nosso modelo está se ajustando em demasia aos dados de treino e não está generalizando bem.\n",
    "- Se ambas curvas estão diminuindo ao final do gráfico, isto pode ser sinal de que paramos o treinamento muito cedo e o modelo pode melhorar.\n",
    "- Se a curva de validação tem erro menor do que a curva do conjunto de treino, isto pode indicar que de que o conjunto de validação é muito mais fácil do que o conjunto de treino.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d308e-316d-402b-af0e-9b9c7c763a0d",
   "metadata": {},
   "source": [
    "<h2>Tarefa de Avaliação de Modelos</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0646b0dc-3a99-4293-a891-6a938feba999",
   "metadata": {},
   "source": [
    "Altere a função do gradiente descendente para que além de armazenar o histórico do valor da função de custo, também armazene o valores parciais dos parâmetros $\\vec{w}$ e $b$ nas variáveis ***historico_W*** (uma lista de vetores de **w**) e ***historico_b*** (uma lista de valores escalares). Com isto, poderemos calcular o valor da função de custo também para os nossos dados de teste.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7672ac-3d9f-4312-ad0d-f3b124f6d98e",
   "metadata": {},
   "source": [
    "Depois que a função do gradiente descendente for atualizada, podemos realizar o treinamento mais uma vez para coletar estes dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7a9d12d5-60e1-476c-9862-b8829cfaacb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m W, b, historico_J, historico_W, historico_b \u001b[39m=\u001b[39m gradiente_descendente(X_treino, Y_treino, W_inicial, b, learning_rate, epocas)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 3)"
     ]
    }
   ],
   "source": [
    "W, b, historico_J, historico_W, historico_b = gradiente_descendente(X_treino, Y_treino, W_inicial, b, learning_rate, epocas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63aa38c-0d56-4fa4-936d-75941208d108",
   "metadata": {},
   "source": [
    "Agora que temos o histórico de valores parciais de $\\vec{w}$ e $b$ durante as diferentes iterações, podemos calcular o erro para os dois datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f3a74e-f97e-4aea-96b1-4bd39515c87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "custo_conjunto_teste = []\n",
    "for epoca in range(len(historico_J)):\n",
    "    w_treino_parcial = historico_w[i]\n",
    "    b_treino_parcial = historico_b[i]\n",
    "    custo_conjunto_teste.append(funcao_custo(X_teste, Y_teste,w_treino_parcial,b_treino_parcial))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa56fe48-8b85-482f-9799-493842fc42e6",
   "metadata": {},
   "source": [
    "Finalmente, podemos plotar em conjunto as curvas de aprendizado no conjunto de treinamento e no conjunto de testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd54068-c40b-43ed-9103-2d23639d86e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel(\"Custo\") #nome do eixo y\n",
    "plt.xlabel(\"Tempo de Treinamento\") #nome do deixo x\n",
    "plt.plot(historico_J,label='Curva de treino')\n",
    "plt.plot(custo_conjunto_teste,label='Curva de teste')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f31db21-419c-41dc-b16b-af7124a1a9ac",
   "metadata": {},
   "source": [
    "Abaixo, realize testes com pelo menos outros dois valores de divisão de dataset (que não seja 75% de treino e 25% de teste), realize novamente o treinamento e calcule novamente o erro para o conjunto de teste e faça plots comparando as curvas de treinamento e de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d383f8d4-e366-4004-b218-8a6736ebe897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
